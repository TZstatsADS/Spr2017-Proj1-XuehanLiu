{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Tutorial (week 2) B: text mining\"\noutput: html_notebook\n    toc: true\n    toc_depth: 2\n---\n\n# Step 0: check and install needed packages. Load the libraries and functions. \n\n```{r, message=FALSE, warning=FALSE}\npackages.used=c(\"rvest\", \"tibble\", \"qdap\", \n                \"sentimentr\", \"gplots\", \"dplyr\",\n                \"tm\", \"syuzhet\", \"factoextra\", \n                \"beeswarm\", \"scales\", \"RColorBrewer\",\n                \"RANN\", \"tm\", \"topicmodels\")\n\n# check packages that need to be installed.\npackages.needed=setdiff(packages.used, \n                        intersect(installed.packages()[,1], \n                                  packages.used))\n# install additional packages\nif(length(packages.needed)>0){\n  install.packages(packages.needed, dependencies = TRUE)\n}\n\n# load packages\nlibrary(\"rvest\")\nlibrary(\"tibble\")\nlibrary(\"qdap\")\nlibrary(\"sentimentr\")\nlibrary(\"gplots\")\nlibrary(\"dplyr\")\nlibrary(\"tm\")\nlibrary(\"syuzhet\")\nlibrary(\"factoextra\")\nlibrary(\"beeswarm\")\nlibrary(\"scales\")\nlibrary(\"RColorBrewer\")\nlibrary(\"RANN\")\nlibrary(\"tm\")\nlibrary(\"topicmodels\")\n\nsource(\"../lib/plotstacked.R\")\nsource(\"../lib/speechFuncs.R\")\n```\nThis notebook was prepared with the following environmental settings.\n\n```{r}\nprint(R.version)\n```\n\n# Step 1: Data harvest: scrap speech URLs from <http://www.presidency.ucsb.edu/>.\n\nFollowing the example of [Jerid Francom](http://francojc.github.io/web-scraping-with-rvest/), we used [Selectorgadget](http://selectorgadget.com/) to choose the links we would like to scrap. For this project, we selected all inaugural addresses of past presidents, nomination speeches of major party candidates and farewell addresses. We also included several public speeches from Donald Trump for our textual analysis of presidential speeches. \n\n```{r, message=FALSE, warning=FALSE}\n### Inauguaral speeches\nmain.page <- read_html(x = \"http://www.presidency.ucsb.edu/inaugurals.php\")\n# Get link URLs\n# f.speechlinks is a function for extracting links from the list of speeches. \ninaug=f.speechlinks(main.page)\n#head(inaug)\nas.Date(inaug[,1], format=\"%B %e, %Y\")\ninaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.\n\n#### Nomination speeches\nmain.page=read_html(\"http://www.presidency.ucsb.edu/nomination.php\")\n# Get link URLs\nnomin <- f.speechlinks(main.page)\n#head(nomin)\n#\n#### Farewell speeches\nmain.page=read_html(\"http://www.presidency.ucsb.edu/farewell_addresses.php\")\n# Get link URLs\nfarewell <- f.speechlinks(main.page)\n#head(farewell)\n```\n\n# Step 2: Using speech metadata posted on <http://www.presidency.ucsb.edu/>, we prepared CSV data sets for the speeches we will scrap. \n\n```{r}\ninaug.list=read.csv(\"/Users/xuehan/Desktop/5243 ADS/Spr2017-Proj1-XuehanLiu/data/inauglist.csv\", stringsAsFactors = FALSE)\nnomin.list=read.csv(\"/Users/xuehan/Desktop/5243 ADS/Spr2017-Proj1-XuehanLiu/data/nominlist.csv\", stringsAsFactors = FALSE)\nfarewell.list=read.csv(\"/Users/xuehan/Desktop/5243 ADS/Spr2017-Proj1-XuehanLiu/data/farewelllist.csv\", stringsAsFactors = FALSE)\n```\n\nWe assemble all scrapped speeches into one list. Note here that we don't have the full text yet, only the links to full text transcripts. \n\n# Step 3: scrap the texts of speeches from the speech URLs.\n\n```{r}\nspeech.list=rbind(inaug.list, nomin.list, farewell.list)\nspeech.list$type=c(rep(\"inaug\", nrow(inaug.list)),\n                   rep(\"nomin\", nrow(nomin.list)),\n                   rep(\"farewell\", nrow(farewell.list)))\nspeech.url=rbind(inaug, nomin, farewell)\nspeech.list=cbind(speech.list, speech.url)\n```\n\nBased on the list of speeches, we scrap the main text part of the transcript's html page. For simple html pages of this kind,  [Selectorgadget](http://selectorgadget.com/) is very convenient for identifying the html node that `rvest` can use to scrap its content. For reproducibility, we also save our scrapped speeches into our local folder as individual speech files. \n\n```{r}\n# Loop over each row in speech.list\nspeech.list$fulltext=NA\nfor(i in seq(nrow(speech.list))) {\n  text <- read_html(speech.list$urls[i]) %>% # load the page\n    html_nodes(\".displaytext\") %>% # isloate the text\n    html_text() # get the text\n  speech.list$fulltext[i]=text\n  # Create the file name\n  filename <- paste0(\"../data/fulltext/\", \n                     speech.list$type[i],\n                     speech.list$File[i], \"-\", \n                     speech.list$Term[i], \".txt\")\n  sink(file = filename) %>% # open file to write \n  cat(text)  # write the file\n  sink() # close the file\n}\n```\n\nTrump, as president-elect that has not been a politician, do not have a lot of formal speeches yet. For our textual analysis, we manually add several public transcripts from Trump:\n+ [Transcript: Donald Trump's full immigration speech, annotated. LA Times, 08/31/2016] (http://www.latimes.com/politics/la-na-pol-donald-trump-immigration-speech-transcript-20160831-snap-htmlstory.html)\n+ [Transcript of Donald Trumpâ€™s speech on national security in Philadelphia\n- The Hill, 09/07/16](http://thehill.com/blogs/pundits-blog/campaign/294817-transcript-of-donald-trumps-speech-on-national-security-in)\n+ [Transcript of President-elect Trump's news conference\nCNBC, 01/11/2017](http://www.cnbc.com/2017/01/11/transcript-of-president-elect-donald-j-trumps-news-conference.html)\n\n```{r}\nspeech1=paste(readLines(\"../data/fulltext/SpeechDonaldTrump-NA.txt\", \n                  n=-1, skipNul=TRUE),\n              collapse=\" \")\nspeech2=paste(readLines(\"../data/fulltext/SpeechDonaldTrump-NA2.txt\", \n                  n=-1, skipNul=TRUE),\n              collapse=\" \")\nspeech3=paste(readLines(\"../data/fulltext/PressDonaldTrump-NA.txt\", \n                  n=-1, skipNul=TRUE),\n              collapse=\" \")\n\nTrump.speeches=data.frame(\n  President=rep(\"Donald J. Trump\", 3),\n  File=rep(\"DonaldJTrump\", 3),\n  Term=rep(0, 3),\n  Party=rep(\"Republican\", 3),\n  Date=c(\"August 31, 2016\", \"September 7, 2016\", \"January 11, 2017\"),\n  Words=c(word_count(speech1), word_count(speech2), word_count(speech3)),\n  Win=rep(\"yes\", 3),\n  type=rep(\"speeches\", 3),\n  links=rep(NA, 3),\n  urls=rep(NA, 3),\n  fulltext=c(speech1, speech2, speech3)\n)\n\nspeech.list=rbind(speech.list, Trump.speeches)\n```\n\n# Step 4: data Processing --- generate list of sentences\n\nWe will use sentences as units of analysis for this project, as sentences are natural languge units for organizing thoughts and ideas. For each extracted sentence, we apply sentiment analysis using [NRC sentiment lexion](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). \"The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing.\"\n\nWe assign an sequential id to each sentence in a speech (`sent.id`) and also calculated the number of words in each sentence as *sentence length* (`word.count`).\n\n```{r, message=FALSE, warning=FALSE}\nsentence.list=NULL\nfor(i in 1:nrow(speech.list)){\n  sentences=sent_detect(speech.list$fulltext[i],\n                        endmarks = c(\"?\", \".\", \"!\", \"|\",\";\"))\n  if(length(sentences)>0){\n    emotions=get_nrc_sentiment(sentences)\n    word.count=word_count(sentences)\n    # colnames(emotions)=paste0(\"emo.\", colnames(emotions))\n    # in case the word counts are zeros?\n    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)\n    sentence.list=rbind(sentence.list, \n                        cbind(speech.list[i,-ncol(speech.list)],\n                              sentences=as.character(sentences), \n                              word.count,\n                              emotions,\n                              sent.id=1:length(sentences)\n                              )\n    )\n  }\n}\n```\n\nSome non-sentences exist in raw data due to erroneous extra end-of sentence marks. \n```{r}\nsentence.list=\n  sentence.list%>%\n  filter(!is.na(word.count)) \n\n```\n\n# Step 5: Data analysis --- length of sentences\n\nFor simpler visualization, we chose a subset of better known presidents or presidential candidates on which to focus our analysis. \n\n```{r}\nsel.comparison=c(\"DonaldJTrump\",\"JohnMcCain\", \"GeorgeBush\", \"MittRomney\", \"GeorgeWBush\",\n                 \"RonaldReagan\",\"AlbertGore,Jr\", \"HillaryClinton\",\"JohnFKerry\", \n                 \"WilliamJClinton\",\"HarrySTruman\", \"BarackObama\", \"LyndonBJohnson\",\n                 \"GeraldRFord\", \"JimmyCarter\", \"DwightDEisenhower\", \"FranklinDRoosevelt\",\n                 \"HerbertHoover\",\"JohnFKennedy\",\"RichardNixon\",\"WoodrowWilson\", \n                 \"AbrahamLincoln\", \"TheodoreRoosevelt\", \"JamesGarfield\", \n                 \"JohnQuincyAdams\", \"UlyssesSGrant\", \"ThomasJefferson\",\n                 \"GeorgeWashington\", \"WilliamHowardTaft\", \"AndrewJackson\",\n                 \"WilliamHenryHarrison\", \"JohnAdams\")\n```\n\n## Overview of sentence length distribution by different types of speeches. \n\n### Nomination speeches \n\nFirst, we look at *nomination acceptance speeches* at major party's national conventions. For relevant to Trump's speeches, we limit our attention to speeches for the first terms of former U.S. presidents.  We noticed that a number of presidents have very short sentences in their nomination acceptance speeches. \n\n#### First term\n\n```{r, fig.width = 3, fig.height = 3}\n\npar(mar=c(4, 11, 2, 2))\n\n#sel.comparison=levels(sentence.list$FileOrdered)\nsentence.list.sel=filter(sentence.list, \n                        type==\"nomin\", Term==1, File%in%sel.comparison)\nsentence.list.sel$File=factor(sentence.list.sel$File)\n\nsentence.list.sel$FileOrdered=reorder(sentence.list.sel$File, \n                                  sentence.list.sel$word.count, \n                                  mean, \n                                  order=T)\n\nbeeswarm(word.count~FileOrdered, \n         data=sentence.list.sel,\n         horizontal = TRUE, \n         pch=16, col=alpha(brewer.pal(9, \"Set1\"), 0.6), \n         cex=0.55, cex.axis=0.8, cex.lab=0.8,\n         spacing=5/nlevels(sentence.list.sel$FileOrdered),\n         las=2, xlab=\"Number of words in a sentence.\", ylab=\"\",\n         main=\"Nomination speeches\")\n\n```\n\n#### Second term\n\n```{r, fig.width = 3, fig.height = 1.3}\n\npar(mar=c(4, 11, 2, 2))\n\n#sel.comparison=levels(sentence.list$FileOrdered)\nsentence.list.sel=filter(sentence.list, \n                        type==\"nomin\", Term==2, File%in%sel.comparison)\nsentence.list.sel$File=factor(sentence.list.sel$File)\n\nsentence.list.sel$FileOrdered=reorder(sentence.list.sel$File, \n                                  sentence.list.sel$word.count, \n                                  mean, \n                                  order=T)\n\nbeeswarm(word.count~FileOrdered, \n         data=sentence.list.sel,\n         horizontal = TRUE, \n         pch=16, col=alpha(brewer.pal(9, \"Set1\"), 0.6), \n         cex=0.55, cex.axis=0.8, cex.lab=0.8,\n         spacing=1.2/nlevels(sentence.list.sel$FileOrdered),\n         las=2, xlab=\"Number of words in a sentence.\", ylab=\"\",\n         main=\"Nomination speeches, 2nd term\")\n\n```\n\nWhat are these short sentences?\n```{r}\nsentence.list%>%\n  filter(File==\"DonaldJTrump\", \n         type==\"nomin\", \n         word.count<=3)%>%\n  select(sentences)%>%sample_n(10)\n\nsentence.list%>%\n  filter(File==\"AlbertGore,Jr\", \n         type==\"nomin\", \n         word.count<=3)%>%\n  select(sentences)%>%sample_n(10)\n\nsentence.list%>%\n  filter(File==\"Clinton\", \n         type==\"nomin\", \n         word.count<=3)%>%\n  select(sentences)\n\nsentence.list%>%\n  filter(File==\"WilliamJClinton\", \n         type==\"nomin\", Term==1,\n         word.count<=3)%>%\n  select(sentences)\n```\n\n\n### Inaugural speeches\n\nWe notice that the sentences in inaugural speeches are longer than those in nomination acceptance speeches. \n\n```{r, fig.width = 3, fig.height = 3}\nsentence.list.sel=sentence.list%>%filter(type==\"inaug\", File%in%sel.comparison, Term==1)\nsentence.list.sel$File=factor(sentence.list.sel$File)\n\nsentence.list.sel$FileOrdered=reorder(sentence.list.sel$File, \n                                  sentence.list.sel$word.count, \n                                  mean, \n                                  order=T)\npar(mar=c(4, 11, 2, 2))\n\nbeeswarm(word.count~FileOrdered, \n         data=sentence.list.sel,\n         horizontal = TRUE,\n         pch=16, col=alpha(brewer.pal(9, \"Set1\"), 0.6), \n         cex=0.55, cex.axis=0.8, cex.lab=0.8,\n         spacing=5/nlevels(sentence.list.sel$FileOrdered),\n         las=2, ylab=\"\", xlab=\"Number of words in a sentence.\",\n         main=\"Inaugural Speeches\")\n```\n\nShort sentences in inaugural speeches. \n```{r}\nsentence.list%>%\n  filter(File==\"BarackObama\", \n         type==\"inaug\", \n         word.count<=3)%>%\n  select(sentences)\n```\n\n\n# Step 5: Data analysis --- sentiment analsis\n\n## Sentence length variation over the course of the speech, with emotions. \n\nHow our presidents (or candidates) alternate between long and short sentences and how they shift between different sentiments in their speeches. It is interesting to note that some presidential candidates' speech are more colorful than others. Here we used the same color theme as in the movie \"Inside Out.\"\n\n![image](http://www.staffordschools.net/cms/lib011/VA01818723/Centricity/Domain/3574/character_icon.png)\n\n```{r, fig.height=2.5, fig.width=2}\npar(mfrow=c(4,1), mar=c(1,0,2,0), bty=\"n\", xaxt=\"n\", yaxt=\"n\", font.main=1)\n\nf.plotsent.len(In.list=sentence.list, InFile=\"HillaryClinton\", \n               InType=\"nomin\", InTerm=1, President=\"Hillary Clinton\")\n\nf.plotsent.len(In.list=sentence.list, InFile=\"DonaldJTrump\", \n               InType=\"nomin\", InTerm=1, President=\"Donald Trump\")\n\nf.plotsent.len(In.list=sentence.list, InFile=\"BarackObama\", \n               InType=\"nomin\", InTerm=1, President=\"Barack Obama\")\n\nf.plotsent.len(In.list=sentence.list, InFile=\"GeorgeWBush\", \n               InType=\"nomin\", InTerm=1, President=\"George W. Bush\")\n```\n\n### What are the emotionally charged sentences?\n\n```{r}\nprint(\"Hillary Clinton\")\nspeech.df=tbl_df(sentence.list)%>%\n  filter(File==\"HillaryClinton\", type==\"nomin\", word.count>=4)%>%\n  select(sentences, anger:trust)\nspeech.df=as.data.frame(speech.df)\nas.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])\n\nprint(\"Barack Obama\")\nspeech.df=tbl_df(sentence.list)%>%\n  filter(File==\"BarackObama\", type==\"nomin\", Term==1, word.count>=5)%>%\n  select(sentences, anger:trust)\nspeech.df=as.data.frame(speech.df)\nas.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])\n\nprint(\"George W Bush\")\nspeech.df=tbl_df(sentence.list)%>%\n  filter(File==\"GeorgeWBush\", type==\"nomin\", Term==1, word.count>=4)%>%\n  select(sentences, anger:trust)\nspeech.df=as.data.frame(speech.df)\nas.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])\n\nprint(\"Donald Trump\")\nspeech.df=tbl_df(sentence.list)%>%\n  filter(File==\"DonaldJTrump\", type==\"nomin\", Term==1, word.count>=5)%>%\n  select(sentences, anger:trust)\nspeech.df=as.data.frame(speech.df)\nas.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])\n\n```\n\n\n## Clustering of emotions\n```{r, fig.width=2, fig.height=2}\nheatmap.2(cor(sentence.list%>%filter(type==\"inaug\")%>%select(anger:trust)), \n          scale = \"none\", \n          col = bluered(100), , margin=c(6, 6), key=F,\n          trace = \"none\", density.info = \"none\")\n\npar(mar=c(4, 6, 2, 1))\nemo.means=colMeans(select(sentence.list, anger:trust)>0.01)\ncol.use=c(\"red2\", \"darkgoldenrod1\", \n            \"chartreuse3\", \"blueviolet\",\n            \"darkgoldenrod2\", \"dodgerblue3\", \n            \"darkgoldenrod1\", \"darkgoldenrod1\")\nbarplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main=\"Inaugural Speeches\")\n```\n\n```{r, fig.height=3.3, fig.width=3.7}\npresid.summary=tbl_df(sentence.list)%>%\n  filter(type==\"nomin\", File%in%sel.comparison)%>%\n  #group_by(paste0(type, File))%>%\n  group_by(File)%>%\n  summarise(\n    anger=mean(anger),\n    anticipation=mean(anticipation),\n    disgust=mean(disgust),\n    fear=mean(fear),\n    joy=mean(joy),\n    sadness=mean(sadness),\n    surprise=mean(surprise),\n    trust=mean(trust)\n    #negative=mean(negative),\n    #positive=mean(positive)\n  )\n\npresid.summary=as.data.frame(presid.summary)\nrownames(presid.summary)=as.character((presid.summary[,1]))\nkm.res=kmeans(presid.summary[,-1], iter.max=200,\n              5)\nfviz_cluster(km.res, \n             stand=F, repel= TRUE,\n             data = presid.summary[,-1], xlab=\"\", xaxt=\"n\",\n             show.clust.cent=FALSE)\n```\n\n# Step 5: Data analysis --- Topic modeling\n\nFor topic modeling, we prepare a corpus of sentence snipets as follows. For each speech, we start with sentences and prepare a snipet with a given sentence with the flanking sentences. \n\n```{r}\ncorpus.list=sentence.list[2:(nrow(sentence.list)-1), ]\nsentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]\nsentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]\ncorpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=\" \")\nrm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]\nrm.rows=c(rm.rows, rm.rows-1)\ncorpus.list=corpus.list[-rm.rows, ]\n```\n\n## Text mining\n```{r}\ndocs <- Corpus(VectorSource(corpus.list$snipets))\nwriteLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))\n```\n\n### Text basic processing\nAdapted from <https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/>.\n\n```{r}\n#remove potentially problematic symbols\ndocs <-tm_map(docs,content_transformer(tolower))\nwriteLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))\n\n#remove punctuation\ndocs <- tm_map(docs, removePunctuation)\nwriteLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))\n\n#Strip digits\ndocs <- tm_map(docs, removeNumbers)\nwriteLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))\n\n#remove stopwords\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\nwriteLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))\n\n#remove whitespace\ndocs <- tm_map(docs, stripWhitespace)\nwriteLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))\n\n#Stem document\ndocs <- tm_map(docs,stemDocument)\nwriteLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))\n```\n\n### Topic modeling\n\nGengerate document-term matrices. \n\n```{r}\ndtm <- DocumentTermMatrix(docs)\n#convert rownames to filenames#convert rownames to filenames\nrownames(dtm) <- paste(corpus.list$type, corpus.list$File,\n                       corpus.list$Term, corpus.list$sent.id, sep=\"_\")\n\nrowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document\n\ndtm  <- dtm[rowTotals> 0, ]\ncorpus.list=corpus.list[rowTotals>0, ]\n\n```\n\nRun LDA\n\n```{r}\n#Set parameters for Gibbs sampling\nburnin <- 4000\niter <- 2000\nthin <- 500\nseed <-list(2003,5,63,100001,765)\nnstart <- 5\nbest <- TRUE\n\n#Number of topics\nk <- 15\n\n#Run LDA using Gibbs sampling\nldaOut <-LDA(dtm, k, method=\"Gibbs\", control=list(nstart=nstart, \n                                                 seed = seed, best=best,\n                                                 burnin = burnin, iter = iter, \n                                                 thin=thin))\n#write out results\n#docs to topics\nldaOut.topics <- as.matrix(topics(ldaOut))\ntable(c(1:k, ldaOut.topics))\nwrite.csv(ldaOut.topics,file=paste(\"../out/LDAGibbs\",k,\"DocsToTopics.csv\"))\n\n#top 6 terms in each topic\nldaOut.terms <- as.matrix(terms(ldaOut,20))\nwrite.csv(ldaOut.terms,file=paste(\"../out/LDAGibbs\",k,\"TopicsToTerms.csv\"))\n\n#probabilities associated with each topic assignment\ntopicProbabilities <- as.data.frame(ldaOut@gamma)\nwrite.csv(topicProbabilities,file=paste(\"../out/LDAGibbs\",k,\"TopicProbabilities.csv\"))\n```\n```{r}\nterms.beta=ldaOut@beta\nterms.beta=scale(terms.beta)\ntopics.terms=NULL\nfor(i in 1:k){\n  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])\n}\ntopics.terms\nldaOut.terms\n```\n\nBased on the most popular terms and the most salient terms for each topic, we assign a hashtag to each topic. This part require manual setup as the topics are likely to change. \n\n```{r}\ntopics.hash=c(\"Economy\", \"America\", \"Defense\", \"Belief\", \"Election\", \"Patriotism\", \"Unity\", \"Government\", \"Reform\", \"Temporal\", \"WorkingFamilies\", \"Freedom\", \"Equality\", \"Misc\", \"Legislation\")\ncorpus.list$ldatopic=as.vector(ldaOut.topics)\ncorpus.list$ldahash=topics.hash[ldaOut.topics]\n\ncolnames(topicProbabilities)=topics.hash\ncorpus.list.df=cbind(corpus.list, topicProbabilities)\n```\n\n## Clustering of topics\n```{r, fig.width=3, fig.height=4}\npar(mar=c(1,1,1,1))\ntopic.summary=tbl_df(corpus.list.df)%>%\n              filter(type%in%c(\"nomin\", \"inaug\"), File%in%sel.comparison)%>%\n              select(File, Economy:Legislation)%>%\n              group_by(File)%>%\n              summarise_each(funs(mean))\ntopic.summary=as.data.frame(topic.summary)\nrownames(topic.summary)=topic.summary[,1]\n\n# [1] \"Economy\"         \"America\"         \"Defense\"         \"Belief\"         \n# [5] \"Election\"        \"Patriotism\"      \"Unity\"           \"Government\"     \n# [9] \"Reform\"          \"Temporal\"        \"WorkingFamilies\" \"Freedom\"        \n# [13] \"Equality\"        \"Misc\"            \"Legislation\"       \n\ntopic.plot=c(1, 13, 9, 11, 8, 3, 7)\nprint(topics.hash[topic.plot])\n\nheatmap.2(as.matrix(topic.summary[,topic.plot+1]), \n          scale = \"column\", key=F, \n          col = bluered(100),\n          cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),\n          trace = \"none\", density.info = \"none\")\n```\n\n```{r, fig.width=3.3, fig.height=5}\n# [1] \"Economy\"         \"America\"         \"Defense\"         \"Belief\"         \n# [5] \"Election\"        \"Patriotism\"      \"Unity\"           \"Government\"     \n# [9] \"Reform\"          \"Temporal\"        \"WorkingFamilies\" \"Freedom\"        \n# [13] \"Equality\"        \"Misc\"            \"Legislation\"       \n \n\npar(mfrow=c(5, 1), mar=c(1,1,2,0), bty=\"n\", xaxt=\"n\", yaxt=\"n\")\n\ntopic.plot=c(1, 13, 14, 15, 8, 9, 12)\nprint(topics.hash[topic.plot])\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"GeorgeBush\", type==\"nomin\",Term==1)%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1], \n             xlab=\"Sentences\", ylab=\"Topic share\", main=\"George Bush, Nomination\")\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"WilliamJClinton\", type==\"nomin\", Term==1)%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n            xlab=\"Sentences\", ylab=\"Topic share\", main=\"Bill Clinton, Nomination\")\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"GeorgeWBush\", type==\"nomin\", Term==1)%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1], \n            xlab=\"Sentences\", ylab=\"Topic share\", main=\"George W Bush, Nomination\")\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"BarackObama\", type==\"nomin\", Term==1)%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n            xlab=\"Sentences\", ylab=\"Topic share\", main=\"Barack Obama, Nomination\")\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"DonaldJTrump\", type==\"nomin\")%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n            xlab=\"Sentences\", ylab=\"Topic share\", main=\"Donald Trump, Nomination\")\n```\n\n```{r, fig.width=3.3, fig.height=5}\n# [1] \"Economy\"         \"America\"         \"Defense\"         \"Belief\"         \n# [5] \"Election\"        \"Patriotism\"      \"Unity\"           \"Government\"     \n# [9] \"Reform\"          \"Temporal\"        \"WorkingFamilies\" \"Freedom\"        \n# [13] \"Equality\"        \"Misc\"            \"Legislation\"       \n\n\npar(mfrow=c(5, 1), mar=c(1,1,2,0), bty=\"n\", xaxt=\"n\", yaxt=\"n\")\n\n\ntopic.plot=c(1, 13, 14, 15, 8, 9, 12)\nprint(topics.hash[topic.plot])\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"GeorgeBush\", type==\"inaug\", Term==1)%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n             xlab=\"Sentences\", ylab=\"Topic share\", main=\"George Bush, inaugural Speeches\")\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"WilliamJClinton\", type==\"inaug\", Term==1)%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n             xlab=\"Sentences\", ylab=\"Topic share\", main=\"William J Clinton, inaugural Speeches\")\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"GeorgeWBush\", type==\"inaug\", Term==1)%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n             xlab=\"Sentences\", ylab=\"Topic share\", main=\"George W. Bush, inaugural Speeches\")\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"BarackObama\", type==\"inaug\", Term==1)%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n             xlab=\"Sentences\", ylab=\"Topic share\", main=\"Barack Obama, inaugural Speeches\")\n```\n```{r, fig.width=4, fig.height=5}\n# [1] \"Economy\"         \"America\"         \"Defense\"         \"Belief\"         \n# [5] \"Election\"        \"Patriotism\"      \"Unity\"           \"Government\"     \n# [9] \"Reform\"          \"Temporal\"        \"WorkingFamilies\" \"Freedom\"        \n# [13] \"Equality\"        \"Misc\"            \"Legislation\"       \n\n\npar(mfrow=c(5, 1))\n\ntopic.plot=c(1, 13, 14, 15, 8, 9, 12)\nprint(topics.hash[topic.plot])\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"RonaldReagan\", type==\"farewell\")%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n             xlab=\"Sentences\", ylab=\"Topic share\", main=\"Ronald Reagan, Farewell Speeches\")\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"GeorgeBush\", type==\"farewell\")%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n             xlab=\"Sentences\", ylab=\"Topic share\", main=\"George Bush, Farewell Speeches\")\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"WilliamJClinton\", type==\"farewell\")%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n             xlab=\"Sentences\", ylab=\"Topic share\", main=\"William J. Clinton, Farewell Speeches\")\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"GeorgeWBush\", type==\"farewell\")%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n             xlab=\"Sentences\", ylab=\"Topic share\", main=\"George W Bush, Farewell Speeches\")\n\n\nspeech.df=tbl_df(corpus.list.df)%>%filter(File==\"BarackObama\", type==\"farewell\")%>%select(sent.id, Economy:Legislation)\nspeech.df=as.matrix(speech.df)\nspeech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)\nspeech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])\nplot.stacked(speech.df[,1], speech.df[,topic.plot+1],\n             xlab=\"Sentences\", ylab=\"Topic share\", main=\"Barack Obama, Farewell Speeches\")\n```\n\n```{r}\nspeech.df=tbl_df(corpus.list.df)%>%filter(type==\"nomin\", word.count<20)%>%select(sentences, Economy:Legislation)\n\nas.character(speech.df$sentences[apply(as.data.frame(speech.df[,-1]), 2, which.max)])\n\nnames(speech.df)[-1]\n\n```\n\n\n```{r, fig.width=3, fig.height=3}\npresid.summary=tbl_df(corpus.list.df)%>%\n  filter(type==\"inaug\", File%in%sel.comparison)%>%\n  select(File, Economy:Legislation)%>%\n  group_by(File)%>%\n  summarise_each(funs(mean))\n\npresid.summary=as.data.frame(presid.summary)\nrownames(presid.summary)=as.character((presid.summary[,1]))\nkm.res=kmeans(scale(presid.summary[,-1]), iter.max=200,\n              5)\nfviz_cluster(km.res, \n             stand=T, repel= TRUE,\n             data = presid.summary[,-1],\n             show.clust.cent=FALSE)\n```\n\n",
    "created" : 1485792657854.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2086817756",
    "id" : "6BB8B915",
    "lastKnownWriteTime" : 1486070431,
    "last_content_update" : 1486070823354,
    "path" : "~/Desktop/5243 ADS/Tutorial2/doc/wk2-Tutorial-TextMining.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}